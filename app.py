import io
import re
import tempfile
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
from scipy import stats
import statsmodels.formula.api as smf


# =========================
# App config
# =========================
st.set_page_config(
    page_title="Há»— trá»£ nghiÃªn cá»©u cho bÃ¡c sÄ© gia Ä‘Ã¬nh",
    page_icon="ğŸ”¬",
    layout="wide",
)

APP_TITLE = "Há»— trá»£ nghiÃªn cá»©u cho bÃ¡c sÄ© gia Ä‘Ã¬nh"


# =========================
# FullHD compact UI + FIX title cut
# =========================
st.markdown(
    """
    <style>
    /* ======= FIX: Top safe area to avoid Streamlit toolbar overlay ======= */
    :root { --app-top-safe: 64px; }  /* náº¿u váº«n che, tÄƒng 72px */

    /* ======= Layout width + padding ======= */
    .block-container{
        padding-top: calc(var(--app-top-safe) + 0.75rem) !important; /* FIX: khÃ´ng bá»‹ che */
        padding-bottom: 0.60rem !important;
        padding-left: 0.90rem !important;
        padding-right: 0.90rem !important;
        max-width: 1600px !important;      /* Full HD */
    }

    /* ======= Typography (gá»n hÆ¡n ~80%) ======= */
    h1 {
        font-size: 1.70rem !important;
        margin: 0.0rem 0 0.10rem 0 !important;
        line-height: 2.05rem !important;
    }
    h2 { font-size: 1.25rem !important; margin: 0.35rem 0 0.20rem 0 !important; }
    h3 { font-size: 1.05rem !important; margin: 0.35rem 0 0.20rem 0 !important; }
    p, li, label, div { font-size: 0.95rem; }

    /* ======= Reduce gaps ======= */
    div[data-testid="stVerticalBlock"] { gap: 0.28rem; }
    .stMarkdown { margin-bottom: 0.10rem !important; }
    .stCaptionContainer { margin-top: -0.18rem !important; }

    /* ======= Widgets spacing ======= */
    .stSelectbox, .stMultiSelect, .stTextInput, .stFileUploader, .stRadio, .stCheckbox {
        margin-bottom: 0.15rem !important;
    }

    /* ======= Divider ======= */
    hr { margin: 0.40rem 0 !important; }

    /* ======= Buttons (gá»n hÆ¡n) ======= */
    div.stButton > button{
        width: 100%;
        padding: 8px 10px !important;
        border-radius: 12px !important;
        font-size: 14px !important;
        font-weight: 780 !important;
        border: 1px solid rgba(0,0,0,0.10) !important;
        box-shadow: 0 1px 5px rgba(0,0,0,0.06) !important;
    }

    /* ======= Sidebar compact ======= */
    section[data-testid="stSidebar"] .block-container{
        padding-top: 0.55rem !important;
        padding-left: 0.75rem !important;
        padding-right: 0.75rem !important;
    }
    section[data-testid="stSidebar"] p,
    section[data-testid="stSidebar"] label,
    section[data-testid="stSidebar"] div{
        font-size: 0.90rem !important;
    }

    /* ======= Dataframes ======= */
    .stDataFrame { margin-top: 0.10rem !important; }

    /* ======= Caption dÆ°á»›i stepper gá»n ======= */
    [data-testid="stCaptionContainer"] {
        font-size: 0.80rem !important;
        line-height: 1.05rem !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


# =========================
# Helpers: safe name + hashing
# =========================
def _safe_name(name: str) -> str:
    return re.sub(r"[^a-zA-Z0-9_]+", "_", str(name).strip())[:80] or "file"


def _file_sha256(raw: bytes) -> str:
    return hashlib.sha256(raw).hexdigest()


def _df_sha256(df: pd.DataFrame) -> str:
    h = pd.util.hash_pandas_object(df, index=True).values.tobytes()
    return hashlib.sha256(h).hexdigest()


# =========================
# Read files safely
# =========================
def read_csv_safely(uploaded_file) -> pd.DataFrame:
    raw = uploaded_file.getvalue()
    encodings = ["utf-8-sig", "utf-8", "cp1258", "cp1252", "latin1"]
    last_err = None
    for enc in encodings:
        try:
            return pd.read_csv(io.BytesIO(raw), encoding=enc)
        except Exception as e:
            last_err = e
    raise last_err


def _read_via_tempfile(raw: bytes, suffix: str, reader_fn):
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
            tmp.write(raw)
            tmp_path = tmp.name
        return reader_fn(tmp_path)
    finally:
        if tmp_path:
            try:
                Path(tmp_path).unlink(missing_ok=True)
            except Exception:
                pass


def read_file_safely(uploaded_file) -> Dict[str, pd.DataFrame]:
    name = uploaded_file.name
    ext = Path(name).suffix.lower()
    raw = uploaded_file.getvalue()

    if ext == ".csv":
        return {"data": read_csv_safely(uploaded_file)}

    if ext == ".xlsx":
        xls = pd.ExcelFile(io.BytesIO(raw), engine="openpyxl")
        out: Dict[str, pd.DataFrame] = {}
        for sh in xls.sheet_names:
            out[str(sh)] = pd.read_excel(xls, sheet_name=sh)
        return out

    if ext == ".xls":
        # cáº§n xlrd>=2.0.1
        xls = pd.ExcelFile(io.BytesIO(raw), engine="xlrd")
        out: Dict[str, pd.DataFrame] = {}
        for sh in xls.sheet_names:
            out[str(sh)] = pd.read_excel(xls, sheet_name=sh, engine="xlrd")
        return out

    if ext in [".sav", ".zsav"]:
        # FIX lá»—i BytesIO: Ä‘á»c qua file táº¡m
        df = _read_via_tempfile(raw, ext, pd.read_spss)
        return {"data": df}

    if ext == ".dta":
        df = _read_via_tempfile(raw, ".dta", pd.read_stata)
        return {"data": df}

    if ext == ".rds":
        try:
            import pyreadr  # type: ignore
        except Exception as e:
            raise RuntimeError("Thiáº¿u pyreadr Ä‘á»ƒ Ä‘á»c .rds. CÃ i: pip install pyreadr") from e

        def _read_rds(path: str):
            res = pyreadr.read_r(path)
            out: Dict[str, pd.DataFrame] = {}
            for k, v in res.items():
                if isinstance(v, pd.DataFrame):
                    out[str(k) if k else "data"] = v
            return out

        out = _read_via_tempfile(raw, ".rds", _read_rds)
        if not out:
            raise RuntimeError("File .rds khÃ´ng chá»©a DataFrame (hoáº·c object khÃ´ng há»— trá»£).")
        return out

    raise RuntimeError(f"Äá»‹nh dáº¡ng {ext} chÆ°a Ä‘Æ°á»£c há»— trá»£.")


# =========================
# Type detection
# =========================
def is_categorical(s: pd.Series) -> bool:
    if pd.api.types.is_bool_dtype(s) or pd.api.types.is_object_dtype(s) or pd.api.types.is_categorical_dtype(s):
        return True
    if pd.api.types.is_numeric_dtype(s):
        nunique = s.dropna().nunique()
        if nunique <= 10:
            return True
    return False


def coerce_numeric(s: pd.Series) -> pd.Series:
    return pd.to_numeric(s, errors="coerce")


def var_kind(s: pd.Series, forced: str = "Tá»± Ä‘á»™ng") -> str:
    if forced == "Äá»‹nh lÆ°á»£ng (numeric)":
        return "num"
    if forced == "PhÃ¢n loáº¡i (categorical)":
        return "cat"
    return "cat" if is_categorical(s) else "num"


# =========================
# Summaries
# =========================
def summarize_variable(df: pd.DataFrame, col: str) -> Dict[str, str]:
    s = df[col]
    miss = int(s.isna().sum())
    n = int(len(s))
    nunique = int(s.dropna().nunique())

    if is_categorical(s):
        vc = s.astype("string").value_counts(dropna=True).head(3)
        top = ", ".join([f"{idx} ({val})" for idx, val in vc.items()]) if len(vc) else "-"
        return {"TÃªn biáº¿n": col, "Äáº·c tÃ­nh biáº¿n": f"PhÃ¢n loáº¡i | má»©c={nunique} | thiáº¿u={miss}/{n} | top: {top}"}

    x = coerce_numeric(s)
    x_non = x.dropna()
    if len(x_non) == 0:
        return {"TÃªn biáº¿n": col, "Äáº·c tÃ­nh biáº¿n": f"Äá»‹nh lÆ°á»£ng | thiáº¿u={miss}/{n} | (khÃ´ng Ä‘á»c Ä‘Æ°á»£c sá»‘)"}

    mean = float(x_non.mean())
    sd = float(x_non.std(ddof=1)) if len(x_non) >= 2 else float("nan")
    med = float(x_non.median())
    q1 = float(x_non.quantile(0.25))
    q3 = float(x_non.quantile(0.75))
    return {
        "TÃªn biáº¿n": col,
        "Äáº·c tÃ­nh biáº¿n": f"Äá»‹nh lÆ°á»£ng | thiáº¿u={miss}/{n} | mean={mean:.2f}, SD={sd:.2f} | median={med:.2f} (IQR {q1:.2f}-{q3:.2f})",
    }


def overall_summary(df: pd.DataFrame) -> Dict[str, int]:
    n_rows = int(df.shape[0])
    n_cols = int(df.shape[1])
    missing_cells = int(df.isna().sum().sum())
    numeric_cols = sum([pd.api.types.is_numeric_dtype(df[c]) and (not is_categorical(df[c])) for c in df.columns])
    cat_cols = n_cols - numeric_cols
    return {
        "Sá»‘ dÃ²ng": n_rows,
        "Sá»‘ biáº¿n": n_cols,
        "Biáº¿n Ä‘á»‹nh lÆ°á»£ng": int(numeric_cols),
        "Biáº¿n phÃ¢n loáº¡i": int(cat_cols),
        "Ã” thiáº¿u (NA)": missing_cells,
    }


# =========================
# Assumptions: normality & homogeneity
# =========================
def normality_pvalue(x: np.ndarray) -> float:
    x = x[~np.isnan(x)]
    n = len(x)
    if n < 3:
        return float("nan")
    try:
        if n <= 5000:
            return float(stats.shapiro(x).pvalue)
        return float(stats.normaltest(x).pvalue)
    except Exception:
        return float("nan")


def variance_homogeneity_pvalue(groups: List[np.ndarray]) -> float:
    clean = [g[~np.isnan(g)] for g in groups if len(g[~np.isnan(g)]) >= 2]
    if len(clean) < 2:
        return float("nan")
    try:
        return float(stats.levene(*clean, center="median").pvalue)
    except Exception:
        return float("nan")


def assumption_report_num_by_group(df: pd.DataFrame, y_num: str, group_cat: str) -> dict:
    tmp = df[[y_num, group_cat]].dropna().copy()
    tmp[y_num] = pd.to_numeric(tmp[y_num], errors="coerce")
    tmp = tmp.dropna()

    levels = sorted(tmp[group_cat].astype(str).unique().tolist())
    arrays = []
    norm_p = {}
    ns = {}

    for lv in levels:
        a = tmp.loc[tmp[group_cat].astype(str) == lv, y_num].to_numpy()
        a = a[~np.isnan(a)]
        arrays.append(a)
        ns[lv] = int(len(a))
        norm_p[lv] = normality_pvalue(a)

    lev_p = variance_homogeneity_pvalue(arrays)
    return {"levels": levels, "n": ns, "normality_p": norm_p, "levene_p": lev_p, "total_n": int(tmp.shape[0])}


def _norm_ok(report: dict, alpha: float = 0.05) -> bool:
    for lv, n in report["n"].items():
        if n < 3:
            return False
        p = report["normality_p"].get(lv, float("nan"))
        if np.isnan(p) or p < alpha:
            return False
    return True


def _var_ok(report: dict, alpha: float = 0.05) -> bool:
    p = report.get("levene_p", float("nan"))
    return (not np.isnan(p)) and (p >= alpha)


def _assumption_text(rep: dict) -> str:
    norm = ", ".join(
        [
            f"{k}: p={rep['normality_p'][k]:.4f}" if not np.isnan(rep["normality_p"][k]) else f"{k}: p=NA"
            for k in rep["levels"]
        ]
    )
    lev = rep.get("levene_p", float("nan"))
    lev_s = f"{lev:.4f}" if not np.isnan(lev) else "NA"
    return f"Giáº£ Ä‘á»‹nh: Shapiro theo nhÃ³m [{norm}]; Levene p={lev_s}."


# =========================
# Single-X: suggest + run
# =========================
def suggest_single_x_test(
    df: pd.DataFrame,
    y: str,
    x: str,
    y_forced: str = "Tá»± Ä‘á»™ng",
    x_forced: str = "Tá»± Ä‘á»™ng",
) -> Tuple[str, str, str]:
    yk = var_kind(df[y], y_forced)
    xk = var_kind(df[x], x_forced)

    tmp = df[[y, x]].dropna()
    if tmp.shape[0] < 3:
        return ("KhÃ´ng Ä‘á»§ dá»¯ liá»‡u", "Sau khi loáº¡i NA, sá»‘ dÃ²ng quÃ¡ Ã­t Ä‘á»ƒ kiá»ƒm Ä‘á»‹nh.", "none")

    if yk == "cat" and xk == "cat":
        tab = pd.crosstab(tmp[y].astype(str), tmp[x].astype(str))
        if tab.shape == (2, 2) and (tab.values < 5).any():
            return ("Fisher exact (2x2)", "Báº£ng 2x2 cÃ³ Ã´ nhá» â†’ Æ°u tiÃªn Fisher.", "fisher_2x2")
        return ("Chi-bÃ¬nh phÆ°Æ¡ng (Chi-square)", "X vÃ  Y Ä‘á»u phÃ¢n loáº¡i â†’ Chi-square.", "chisq")

    if yk == "num" and xk == "cat":
        rep = assumption_report_num_by_group(df, y_num=y, group_cat=x)
        n_levels = len(rep["levels"])
        norm_ok = _norm_ok(rep)
        var_ok = _var_ok(rep)

        if n_levels == 2:
            if norm_ok and var_ok:
                return ("t-test (Student)", "2 nhÃ³m, Ä‘áº¡t chuáº©n & phÆ°Æ¡ng sai tÆ°Æ¡ng Ä‘Æ°Æ¡ng â†’ Student.", "ttest_student")
            if norm_ok and (not var_ok):
                return ("t-test (Welch)", "2 nhÃ³m, chuáº©n nhÆ°ng phÆ°Æ¡ng sai khÃ¡c â†’ Welch.", "ttest_welch")
            return ("Mannâ€“Whitney U", "2 nhÃ³m, khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh chuáº©n â†’ Mannâ€“Whitney.", "mwu")

        if norm_ok and var_ok:
            return ("ANOVA má»™t yáº¿u tá»‘", "Nhiá»u nhÃ³m, Ä‘áº¡t chuáº©n & Ä‘á»“ng nháº¥t phÆ°Æ¡ng sai â†’ ANOVA.", "anova")
        return ("Kruskalâ€“Wallis", "Nhiá»u nhÃ³m, khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh â†’ Kruskal.", "kruskal")

    if yk == "cat" and xk == "num":
        rep = assumption_report_num_by_group(df, y_num=x, group_cat=y)
        n_levels = len(rep["levels"])
        norm_ok = _norm_ok(rep)
        var_ok = _var_ok(rep)

        if n_levels == 2:
            if norm_ok and var_ok:
                return ("t-test (Student)", "2 nhÃ³m, Ä‘áº¡t chuáº©n & phÆ°Æ¡ng sai tÆ°Æ¡ng Ä‘Æ°Æ¡ng â†’ Student.", "ttest_student_swapped")
            if norm_ok and (not var_ok):
                return ("t-test (Welch)", "2 nhÃ³m, chuáº©n nhÆ°ng phÆ°Æ¡ng sai khÃ¡c â†’ Welch.", "ttest_welch_swapped")
            return ("Mannâ€“Whitney U", "2 nhÃ³m, khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh chuáº©n â†’ Mannâ€“Whitney.", "mwu_swapped")

        if norm_ok and var_ok:
            return ("ANOVA má»™t yáº¿u tá»‘", "Nhiá»u nhÃ³m, Ä‘áº¡t chuáº©n & Ä‘á»“ng nháº¥t phÆ°Æ¡ng sai â†’ ANOVA.", "anova_swapped")
        return ("Kruskalâ€“Wallis", "Nhiá»u nhÃ³m, khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh â†’ Kruskal.", "kruskal_swapped")

    if yk == "num" and xk == "num":
        tmp2 = df[[y, x]].copy()
        tmp2[y] = coerce_numeric(tmp2[y])
        tmp2[x] = coerce_numeric(tmp2[x])
        tmp2 = tmp2.dropna()
        if tmp2.shape[0] < 3:
            return ("KhÃ´ng Ä‘á»§ dá»¯ liá»‡u", "KhÃ´ng Ä‘á»§ dÃ²ng sá»‘ Ä‘á»ƒ tÃ­nh tÆ°Æ¡ng quan.", "none")

        pny = normality_pvalue(tmp2[y].to_numpy())
        pnx = normality_pvalue(tmp2[x].to_numpy())
        if (not np.isnan(pny)) and (not np.isnan(pnx)) and (pny >= 0.05) and (pnx >= 0.05):
            return ("TÆ°Æ¡ng quan Pearson", "X vÃ  Y gáº§n chuáº©n â†’ Pearson.", "corr_pearson")
        return ("TÆ°Æ¡ng quan Spearman", "X hoáº·c Y khÃ´ng chuáº©n/ordinal â†’ Spearman.", "corr_spearman")

    return ("KhÃ´ng xÃ¡c Ä‘á»‹nh", "KhÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c phÃ©p kiá»ƒm phÃ¹ há»£p.", "none")


def _cohens_d(a: np.ndarray, b: np.ndarray) -> float:
    a = a[~np.isnan(a)]
    b = b[~np.isnan(b)]
    if len(a) < 2 or len(b) < 2:
        return float("nan")
    va = np.var(a, ddof=1)
    vb = np.var(b, ddof=1)
    sp = np.sqrt(((len(a) - 1) * va + (len(b) - 1) * vb) / (len(a) + len(b) - 2))
    if sp == 0:
        return float("nan")
    return (np.mean(a) - np.mean(b)) / sp


def _cramers_v(tab: pd.DataFrame) -> float:
    chi2, p, dof, exp = stats.chi2_contingency(tab.values)
    n = tab.values.sum()
    if n == 0:
        return float("nan")
    r, k = tab.shape
    return np.sqrt(chi2 / (n * (min(r, k) - 1))) if min(r, k) > 1 else float("nan")


def run_single_x_test(df: pd.DataFrame, y: str, x: str, test_kind: str) -> Tuple[pd.DataFrame, str]:
    if test_kind == "chisq":
        tmp = df[[y, x]].dropna()
        tab = pd.crosstab(tmp[y].astype(str), tmp[x].astype(str))
        chi2, p, dof, exp = stats.chi2_contingency(tab.values)
        v = _cramers_v(tab)
        out = pd.DataFrame({"Chá»‰ sá»‘": ["Chi2", "df", "p-value", "Cramer's V"], "GiÃ¡ trá»‹": [chi2, dof, p, v]})
        interp = "Diá»…n giáº£i: p nhá» â†’ gá»£i Ã½ cÃ³ liÃªn quan. Cramer's V Ä‘Ã¡nh giÃ¡ Ä‘á»™ máº¡nh liÃªn quan."
        return out, interp

    if test_kind == "fisher_2x2":
        tmp = df[[y, x]].dropna()
        tab = pd.crosstab(tmp[y].astype(str), tmp[x].astype(str))
        if tab.shape != (2, 2):
            raise ValueError("Fisher exact chá»‰ Ã¡p dá»¥ng báº£ng 2x2.")
        oddsratio, p = stats.fisher_exact(tab.values)
        out = pd.DataFrame({"Chá»‰ sá»‘": ["Odds ratio", "p-value"], "GiÃ¡ trá»‹": [oddsratio, p]})
        interp = "Diá»…n giáº£i: p nhá» â†’ gá»£i Ã½ liÃªn quan. OR diá»…n giáº£i theo nhÃ³m tham chiáº¿u."
        return out, interp

    if test_kind in ("ttest_student", "ttest_welch", "mwu", "anova", "kruskal"):
        tmp = df[[y, x]].dropna().copy()
        tmp[y] = coerce_numeric(tmp[y])
        tmp = tmp.dropna()
        groups = tmp[x].astype(str)
        levels = sorted(groups.unique().tolist())
        arrays = [tmp.loc[groups == lv, y].to_numpy() for lv in levels]
        rep = assumption_report_num_by_group(df, y_num=y, group_cat=x)
        assump = _assumption_text(rep)

        if test_kind in ("ttest_student", "ttest_welch"):
            if len(levels) != 2:
                raise ValueError("t-test cáº§n Ä‘Ãºng 2 nhÃ³m.")
            a, b = arrays[0], arrays[1]
            equal_var = (test_kind == "ttest_student")
            tstat, p = stats.ttest_ind(a, b, equal_var=equal_var, nan_policy="omit")
            d = _cohens_d(a, b)
            out = pd.DataFrame({"Chá»‰ sá»‘": ["t", "p-value", "Cohen's d"], "GiÃ¡ trá»‹": [tstat, p, d]})
            interp = f"{assump}\nDiá»…n giáº£i: p nhá» â†’ trung bÃ¬nh khÃ¡c nhau giá»¯a 2 nhÃ³m. Cohenâ€™s d lÃ  effect size."
            return out, interp

        if test_kind == "mwu":
            if len(levels) != 2:
                raise ValueError("Mannâ€“Whitney cáº§n Ä‘Ãºng 2 nhÃ³m.")
            a, b = arrays[0], arrays[1]
            u, p = stats.mannwhitneyu(a, b, alternative="two-sided")
            out = pd.DataFrame({"Chá»‰ sá»‘": ["U", "p-value"], "GiÃ¡ trá»‹": [u, p]})
            interp = f"{assump}\nDiá»…n giáº£i: dÃ¹ng khi khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh chuáº©n."
            return out, interp

        if test_kind == "anova":
            f, p = stats.f_oneway(*arrays)
            out = pd.DataFrame({"Chá»‰ sá»‘": ["F", "p-value"], "GiÃ¡ trá»‹": [f, p]})
            interp = f"{assump}\nDiá»…n giáº£i: p nhá» â†’ cÃ³ Ã­t nháº¥t 1 nhÃ³m khÃ¡c trung bÃ¬nh; nÃªn lÃ m post-hoc."
            return out, interp

        if test_kind == "kruskal":
            h, p = stats.kruskal(*arrays)
            out = pd.DataFrame({"Chá»‰ sá»‘": ["H (Kruskal)", "p-value"], "GiÃ¡ trá»‹": [h, p]})
            interp = f"{assump}\nDiá»…n giáº£i: dÃ¹ng khi khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh; náº¿u cÃ³ Ã½ nghÄ©a nÃªn post-hoc."
            return out, interp

    if test_kind.endswith("_swapped"):
        tmp = df[[y, x]].dropna().copy()
        tmp[x] = coerce_numeric(tmp[x])
        tmp = tmp.dropna()
        groups = tmp[y].astype(str)
        levels = sorted(groups.unique().tolist())
        arrays = [tmp.loc[groups == lv, x].to_numpy() for lv in levels]
        rep = assumption_report_num_by_group(df, y_num=x, group_cat=y)
        assump = _assumption_text(rep)
        base = test_kind.replace("_swapped", "")

        if base in ("ttest_student", "ttest_welch"):
            if len(levels) != 2:
                raise ValueError("t-test cáº§n Ä‘Ãºng 2 nhÃ³m.")
            a, b = arrays[0], arrays[1]
            equal_var = (base == "ttest_student")
            tstat, p = stats.ttest_ind(a, b, equal_var=equal_var, nan_policy="omit")
            d = _cohens_d(a, b)
            out = pd.DataFrame({"Chá»‰ sá»‘": ["t", "p-value", "Cohen's d"], "GiÃ¡ trá»‹": [tstat, p, d]})
            interp = f"{assump}\nDiá»…n giáº£i: p nhá» â†’ trung bÃ¬nh khÃ¡c nhau giá»¯a 2 nhÃ³m (theo Y)."
            return out, interp

        if base == "mwu":
            if len(levels) != 2:
                raise ValueError("Mannâ€“Whitney cáº§n Ä‘Ãºng 2 nhÃ³m.")
            a, b = arrays[0], arrays[1]
            u, p = stats.mannwhitneyu(a, b, alternative="two-sided")
            out = pd.DataFrame({"Chá»‰ sá»‘": ["U", "p-value"], "GiÃ¡ trá»‹": [u, p]})
            interp = f"{assump}\nDiá»…n giáº£i: dÃ¹ng khi khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh chuáº©n."
            return out, interp

        if base == "anova":
            f, p = stats.f_oneway(*arrays)
            out = pd.DataFrame({"Chá»‰ sá»‘": ["F", "p-value"], "GiÃ¡ trá»‹": [f, p]})
            interp = f"{assump}\nDiá»…n giáº£i: p nhá» â†’ cÃ³ Ã­t nháº¥t 1 nhÃ³m khÃ¡c trung bÃ¬nh; nÃªn lÃ m post-hoc."
            return out, interp

        if base == "kruskal":
            h, p = stats.kruskal(*arrays)
            out = pd.DataFrame({"Chá»‰ sá»‘": ["H (Kruskal)", "p-value"], "GiÃ¡ trá»‹": [h, p]})
            interp = f"{assump}\nDiá»…n giáº£i: dÃ¹ng khi khÃ´ng Ä‘áº¡t giáº£ Ä‘á»‹nh; náº¿u cÃ³ Ã½ nghÄ©a nÃªn post-hoc."
            return out, interp

    if test_kind == "corr_pearson":
        tmp = df[[y, x]].copy()
        tmp[y] = coerce_numeric(tmp[y])
        tmp[x] = coerce_numeric(tmp[x])
        tmp = tmp.dropna()
        r, p = stats.pearsonr(tmp[x].to_numpy(), tmp[y].to_numpy())
        out = pd.DataFrame({"Chá»‰ sá»‘": ["Pearson r", "p-value", "n"], "GiÃ¡ trá»‹": [r, p, tmp.shape[0]]})
        interp = "Diá»…n giáº£i: r gáº§n 0 â†’ yáº¿u; gáº§n Â±1 â†’ máº¡nh. p nhá» â†’ liÃªn quan tuyáº¿n tÃ­nh cÃ³ Ã½ nghÄ©a."
        return out, interp

    if test_kind == "corr_spearman":
        tmp = df[[y, x]].copy()
        tmp[y] = coerce_numeric(tmp[y])
        tmp[x] = coerce_numeric(tmp[x])
        tmp = tmp.dropna()
        rho, p = stats.spearmanr(tmp[x].to_numpy(), tmp[y].to_numpy())
        out = pd.DataFrame({"Chá»‰ sá»‘": ["Spearman rho", "p-value", "n"], "GiÃ¡ trá»‹": [rho, p, tmp.shape[0]]})
        interp = "Diá»…n giáº£i: Spearman phÃ¹ há»£p khi dá»¯ liá»‡u khÃ´ng chuáº©n/ordinal."
        return out, interp

    raise ValueError("KhÃ´ng cÃ³ kiá»ƒm Ä‘á»‹nh phÃ¹ há»£p (test_kind=none).")


# =========================
# Model: suggest + build + run
# =========================
def suggest_model(df: pd.DataFrame, y: str, xs: List[str]) -> Tuple[str, str]:
    y_s = df[y]
    if is_categorical(y_s):
        n_levels = int(y_s.dropna().nunique())
        if n_levels <= 1:
            return ("KhÃ´ng Ä‘á»§ dá»¯ liá»‡u", "Y chá»‰ cÃ³ 0â€“1 má»©c sau khi loáº¡i thiáº¿u. HÃ£y kiá»ƒm tra dá»¯ liá»‡u.")
        if n_levels == 2:
            return ("Há»“i quy Logistic nhá»‹ phÃ¢n (Binary Logistic)", "Y phÃ¢n loáº¡i 2 má»©c â†’ logistic nhá»‹ phÃ¢n Ä‘á»ƒ Æ°á»›c lÆ°á»£ng OR.")
        return ("Há»“i quy Logistic Ä‘a danh (Multinomial Logistic)", f"Y >2 má»©c (má»©c={n_levels}) â†’ logistic Ä‘a danh.")
    return ("Há»“i quy tuyáº¿n tÃ­nh (OLS)", "Y Ä‘á»‹nh lÆ°á»£ng â†’ há»“i quy tuyáº¿n tÃ­nh (OLS).")


def build_formula(
    df: pd.DataFrame,
    y: str,
    xs: List[str],
    y_binary_event: Optional[str] = None,
) -> Tuple[str, pd.DataFrame, str]:
    tmp = df[[y] + xs].copy().dropna()

    if is_categorical(tmp[y]):
        n_levels = int(tmp[y].nunique())

        if n_levels == 2:
            y_cat = tmp[y].astype("category")
            cats = list(y_cat.cat.categories)
            event = y_binary_event if (y_binary_event in cats) else cats[1]
            tmp["_y01_"] = (tmp[y] == event).astype(int)

            terms = []
            for x in xs:
                terms.append(f"C(Q('{x}'))" if is_categorical(tmp[x]) else f"Q('{x}')")

            formula = "_y01_ ~ " + " + ".join(terms)
            return formula, tmp, f"logit||Logistic nhá»‹ phÃ¢n: sá»± kiá»‡n (Y=1)='{event}'"

        tmp["_ycat_"] = tmp[y].astype("category")
        tmp["_ycode_"] = tmp["_ycat_"].cat.codes

        terms = []
        for x in xs:
            terms.append(f"C(Q('{x}'))" if is_categorical(tmp[x]) else f"Q('{x}')")

        formula = "_ycode_ ~ " + " + ".join(terms)
        return formula, tmp, "mnlogit||Multinomial: há»‡ sá»‘ theo nhÃ³m tham chiáº¿u"

    tmp[y] = coerce_numeric(tmp[y])
    tmp = tmp.dropna()

    terms = []
    for x in xs:
        terms.append(f"C(Q('{x}'))" if is_categorical(tmp[x]) else f"Q('{x}')")

    formula = f"Q('{y}') ~ " + " + ".join(terms)
    return formula, tmp, "ols||OLS"


def run_model(formula: str, data_used: pd.DataFrame, model_kind: str):
    kind, note = model_kind.split("||", 1)
    if kind == "ols":
        return smf.ols(formula=formula, data=data_used).fit(), note
    if kind == "logit":
        return smf.logit(formula=formula, data=data_used).fit(disp=0), note
    if kind == "mnlogit":
        return smf.mnlogit(formula=formula, data=data_used).fit(disp=0), note
    raise ValueError("Unknown model kind")


def ols_table(fit) -> pd.DataFrame:
    conf = fit.conf_int()
    out = pd.DataFrame({"Há»‡ sá»‘": fit.params, "CI 2.5%": conf[0], "CI 97.5%": conf[1], "p-value": fit.pvalues})
    out.index.name = "Biáº¿n"
    return out.sort_values("p-value")


def logit_or_table(fit) -> pd.DataFrame:
    conf = fit.conf_int()
    out = pd.DataFrame(
        {"OR": np.exp(fit.params), "CI 2.5%": np.exp(conf[0]), "CI 97.5%": np.exp(conf[1]), "p-value": fit.pvalues}
    )
    out.index.name = "Biáº¿n"
    return out.sort_values("p-value")


# =========================
# Session state
# =========================
if "datasets" not in st.session_state:
    st.session_state["datasets"] = {}
if "active_name" not in st.session_state:
    st.session_state["active_name"] = None

if "pending_tables" not in st.session_state:
    st.session_state["pending_tables"] = None
if "pending_fname" not in st.session_state:
    st.session_state["pending_fname"] = None
if "pending_file_hash" not in st.session_state:
    st.session_state["pending_file_hash"] = None

if "hash_to_key" not in st.session_state:
    st.session_state["hash_to_key"] = {}
if "key_to_hashes" not in st.session_state:
    st.session_state["key_to_hashes"] = {}

if "last_upload_hash" not in st.session_state:
    st.session_state["last_upload_hash"] = None

if "last_result" not in st.session_state:
    st.session_state["last_result"] = None
if "last_run_meta" not in st.session_state:
    st.session_state["last_run_meta"] = None

if "active_step" not in st.session_state:
    st.session_state["active_step"] = 1


def _register_dataset(key: str, df: pd.DataFrame, hashes: List[str]):
    st.session_state["datasets"][key] = df
    st.session_state["active_name"] = key
    st.session_state["key_to_hashes"].setdefault(key, set())
    for h in hashes:
        st.session_state["hash_to_key"][h] = key
        st.session_state["key_to_hashes"][key].add(h)


def _delete_dataset(key: str):
    st.session_state["datasets"].pop(key, None)
    hashes = st.session_state["key_to_hashes"].pop(key, set())
    for h in list(hashes):
        if st.session_state["hash_to_key"].get(h) == key:
            st.session_state["hash_to_key"].pop(h, None)


# =========================
# Header (compact, safe top)
# =========================
st.markdown(
    f"""
    <div style="padding:0.10rem 0 0.10rem 0; margin-top:0.20rem;">
      <h1 style="margin:0;">{APP_TITLE}</h1>
      <div style="color:#6b7280; font-size:0.88rem; margin-top:0.08rem;">
        Upload dá»¯ liá»‡u â†’ chá»n biáº¿n â†’ kiá»ƒm Ä‘á»‹nh (1 X) hoáº·c mÃ´ hÃ¬nh (nhiá»u X) â†’ káº¿t quáº£ + giáº£i thÃ­ch
      </div>
    </div>
    """,
    unsafe_allow_html=True,
)

# =========================
# Sidebar (upload + dataset)
# =========================
with st.sidebar:
    st.markdown("## â¬†ï¸ Upload")
    up = st.file_uploader(
        "Táº£i lÃªn dá»¯ liá»‡u (CSV/XLSX/XLS/SAV/ZsAV/DTA/RDS)",
        type=["csv", "xlsx", "xls", "sav", "zsav", "dta", "rds"],
        accept_multiple_files=False,
    )

    if up is not None:
        try:
            raw = up.getvalue()
            file_hash = _file_sha256(raw)

            if st.session_state["last_upload_hash"] != file_hash:
                st.session_state["last_upload_hash"] = file_hash

                if file_hash in st.session_state["hash_to_key"]:
                    existed_key = st.session_state["hash_to_key"][file_hash]
                    st.session_state["active_name"] = existed_key
                    st.info(f"ÄÃ£ cÃ³ trÆ°á»›c Ä‘Ã³ â†’ {existed_key}")
                else:
                    tables = read_file_safely(up)

                    if len(tables) > 1:
                        st.session_state["pending_tables"] = tables
                        st.session_state["pending_fname"] = up.name
                        st.session_state["pending_file_hash"] = file_hash
                        st.info("File cÃ³ nhiá»u báº£ng â†’ chá»n 1 báº£ng Ä‘á»ƒ nháº­p.")
                    else:
                        df_new = list(tables.values())[0]
                        base = _safe_name(Path(up.name).stem)
                        key = base
                        i = 2
                        while key in st.session_state["datasets"]:
                            key = f"{base}_{i}"
                            i += 1

                        df_hash = _df_sha256(df_new)
                        _register_dataset(key, df_new, hashes=[file_hash, df_hash])
                        st.session_state["active_step"] = 1
                        st.success(f"ÄÃ£ táº£i: {key}")

        except Exception as e:
            st.error(f"KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file: {e}")

    if st.session_state["pending_tables"] is not None:
        st.markdown("### Chá»n sheet/object")
        tables = st.session_state["pending_tables"]
        fname = st.session_state["pending_fname"] or "file"
        pending_file_hash = st.session_state["pending_file_hash"]

        chosen_table = st.selectbox("Sheet/Object", options=list(tables.keys()))
        c1, c2 = st.columns([1, 1], gap="small")

        with c1:
            if st.button("Nháº­p", use_container_width=True):
                df_new = tables[chosen_table]
                table_hash = _df_sha256(df_new)

                if table_hash in st.session_state["hash_to_key"]:
                    existed_key = st.session_state["hash_to_key"][table_hash]
                    st.session_state["active_name"] = existed_key
                    st.info(f"ÄÃ£ nháº­p trÆ°á»›c Ä‘Ã³ â†’ {existed_key}")
                else:
                    base = _safe_name(Path(fname).stem)
                    sh = _safe_name(chosen_table)
                    key_base = f"{base}__{sh}"
                    key = key_base
                    i = 2
                    while key in st.session_state["datasets"]:
                        key = f"{key_base}_{i}"
                        i += 1

                    hashes = [table_hash]
                    if pending_file_hash:
                        hashes.append(pending_file_hash)

                    _register_dataset(key, df_new, hashes=hashes)
                    st.session_state["active_step"] = 1
                    st.success(f"ÄÃ£ nháº­p: {key}")

                st.session_state["pending_tables"] = None
                st.session_state["pending_fname"] = None
                st.session_state["pending_file_hash"] = None
                st.rerun()

        with c2:
            if st.button("Huá»·", use_container_width=True):
                st.session_state["pending_tables"] = None
                st.session_state["pending_fname"] = None
                st.session_state["pending_file_hash"] = None
                st.rerun()

    st.markdown("---")
    st.markdown("## ğŸ“ Dataset")

    names_all = list(st.session_state["datasets"].keys())
    if not names_all:
        st.info("ChÆ°a cÃ³ dá»¯ liá»‡u.")
        st.stop()

    ds_q = st.text_input("TÃ¬m dataset", value="", placeholder="gÃµ tÃªn dataset...")
    if ds_q.strip():
        names = [n for n in names_all if ds_q.lower() in n.lower()] or names_all
    else:
        names = names_all

    active = st.session_state["active_name"] or names_all[0]
    if active not in names_all:
        active = names_all[0]
        st.session_state["active_name"] = active

    chosen = st.selectbox("Chá»n dataset", options=names, index=names.index(active) if active in names else 0)
    st.session_state["active_name"] = chosen

    with st.expander("âœï¸ Äá»•i tÃªn dataset"):
        new_name = st.text_input("TÃªn má»›i", value=chosen)
        if st.button("LÆ°u tÃªn", use_container_width=True):
            new_name = _safe_name(new_name)
            if (new_name != chosen) and (new_name in st.session_state["datasets"]):
                st.error("TÃªn Ä‘Ã£ tá»“n táº¡i.")
            else:
                df_tmp = st.session_state["datasets"].pop(chosen)
                st.session_state["datasets"][new_name] = df_tmp

                hashes = st.session_state["key_to_hashes"].pop(chosen, set())
                st.session_state["key_to_hashes"][new_name] = hashes
                for h in list(hashes):
                    if st.session_state["hash_to_key"].get(h) == chosen:
                        st.session_state["hash_to_key"][h] = new_name

                st.session_state["active_name"] = new_name
                st.success("ÄÃ£ Ä‘á»•i tÃªn.")
                st.rerun()

    df_active = st.session_state["datasets"][st.session_state["active_name"]]
    summ_side = overall_summary(df_active)
    st.caption(f"rows={summ_side['Sá»‘ dÃ²ng']} | biáº¿n={summ_side['Sá»‘ biáº¿n']} | thiáº¿u={summ_side['Ã” thiáº¿u (NA)']}")

    c1, c2 = st.columns([1, 1], gap="small")
    with c1:
        if st.button("XoÃ¡", use_container_width=True):
            _delete_dataset(chosen)
            remaining = list(st.session_state["datasets"].keys())
            st.session_state["active_name"] = remaining[0] if remaining else None
            st.session_state["last_result"] = None
            st.session_state["last_run_meta"] = None
            st.session_state["active_step"] = 1
            st.rerun()

    with c2:
        if st.button("XoÃ¡ háº¿t", use_container_width=True):
            st.session_state["datasets"] = {}
            st.session_state["active_name"] = None
            st.session_state["pending_tables"] = None
            st.session_state["pending_fname"] = None
            st.session_state["pending_file_hash"] = None
            st.session_state["hash_to_key"] = {}
            st.session_state["key_to_hashes"] = {}
            st.session_state["last_upload_hash"] = None
            st.session_state["last_result"] = None
            st.session_state["last_run_meta"] = None
            st.session_state["active_step"] = 1
            st.rerun()


# =========================
# Main data
# =========================
df = st.session_state["datasets"][st.session_state["active_name"]]
cols = df.columns.tolist()


# =========================
# Stepper
# =========================
st.markdown("## ğŸ§­ CÃ¡c bÆ°á»›c")
b1, b2, b3 = st.columns(3, gap="small")

with b1:
    t = "primary" if st.session_state["active_step"] == 1 else "secondary"
    if st.button("1) ğŸ“„ Dá»¯ liá»‡u", type=t, use_container_width=True):
        st.session_state["active_step"] = 1
        st.rerun()
    st.caption("Tá»•ng quan â€¢ xem báº£ng â€¢ danh sÃ¡ch biáº¿n")

with b2:
    t = "primary" if st.session_state["active_step"] == 2 else "secondary"
    if st.button("2) ğŸ¯ Chá»n biáº¿n", type=t, use_container_width=True):
        st.session_state["active_step"] = 2
        st.rerun()
    st.caption("Chá»n Y/X â€¢ gá»£i Ã½ â€¢ báº¥m Run")

with b3:
    t = "primary" if st.session_state["active_step"] == 3 else "secondary"
    if st.button("3) ğŸ“Œ Káº¿t quáº£", type=t, use_container_width=True):
        st.session_state["active_step"] = 3
        st.rerun()
    st.caption("Báº£ng â€¢ biá»ƒu Ä‘á»“ â€¢ diá»…n giáº£i")

st.divider()


# =========================
# STEP 1: Data
# =========================
if st.session_state["active_step"] == 1:
    st.subheader("ğŸ“„ Dá»¯ liá»‡u")
    summ = overall_summary(df)
    m1, m2, m3, m4, m5 = st.columns(5, gap="small")
    m1.metric("DÃ²ng", summ["Sá»‘ dÃ²ng"])
    m2.metric("Biáº¿n", summ["Sá»‘ biáº¿n"])
    m3.metric("Äá»‹nh lÆ°á»£ng", summ["Biáº¿n Ä‘á»‹nh lÆ°á»£ng"])
    m4.metric("PhÃ¢n loáº¡i", summ["Biáº¿n phÃ¢n loáº¡i"])
    m5.metric("NA", summ["Ã” thiáº¿u (NA)"])

    cL, cR = st.columns([1.2, 1.0], gap="small")
    with cL:
        st.markdown("### ğŸ‘€ Xem nhanh")
        st.dataframe(df.head(25), use_container_width=True, height=270)
    with cR:
        st.markdown("### ğŸ§¾ Danh sÃ¡ch biáº¿n")
        q = st.text_input("TÃ¬m biáº¿n", value="", placeholder="vd: age, weight...")
        filter_opt = st.selectbox("Lá»c", ["Táº¥t cáº£", "Chá»‰ Ä‘á»‹nh lÆ°á»£ng", "Chá»‰ phÃ¢n loáº¡i"], index=0)
        var_rows = [summarize_variable(df, c) for c in cols]
        var_df = pd.DataFrame(var_rows)
        if q.strip():
            var_df = var_df[var_df["TÃªn biáº¿n"].str.contains(q.strip(), case=False, na=False)].copy()
        if filter_opt == "Chá»‰ Ä‘á»‹nh lÆ°á»£ng":
            var_df = var_df[var_df["Äáº·c tÃ­nh biáº¿n"].str.contains("Äá»‹nh lÆ°á»£ng", na=False)]
        elif filter_opt == "Chá»‰ phÃ¢n loáº¡i":
            var_df = var_df[var_df["Äáº·c tÃ­nh biáº¿n"].str.contains("PhÃ¢n loáº¡i", na=False)]
        st.dataframe(var_df, use_container_width=True, height=270)

    st.info("ğŸ‘‰ Sang **2) Chá»n biáº¿n** Ä‘á»ƒ chá»n Y/X vÃ  báº¥m Run.")


# =========================
# STEP 2: Choose variables (placeholder)
# =========================
elif st.session_state["active_step"] == 2:
    st.subheader("ğŸ¯ Chá»n biáº¿n")
    st.info("Báº¡n Ä‘ang á»Ÿ bÆ°á»›c 2. (Náº¿u báº¡n muá»‘n mÃ¬nh gá»­i láº¡i báº£n cÃ³ Ä‘á»§ mÃ´ hÃ¬nh + káº¿t quáº£ nhÆ° file trÆ°á»›c, nÃ³i mÃ¬nh biáº¿t.)")


# =========================
# STEP 3: Results (placeholder)
# =========================
else:
    st.subheader("ğŸ“Œ Káº¿t quáº£")
    st.info("Báº¡n Ä‘ang á»Ÿ bÆ°á»›c 3. (Náº¿u báº¡n muá»‘n mÃ¬nh gá»­i láº¡i báº£n Ä‘áº§y Ä‘á»§ pháº§n cháº¡y test/mÃ´ hÃ¬nh nhÆ° file trÆ°á»›c, nÃ³i mÃ¬nh biáº¿t.)")

st.divider()
st.caption(
    "âš ï¸ LÆ°u Ã½: CÃ´ng cá»¥ há»— trá»£ gá»£i Ã½ vÃ  cháº¡y kiá»ƒm Ä‘á»‹nh/mÃ´ hÃ¬nh cÆ¡ báº£n. "
    "NgÆ°á»i dÃ¹ng cáº§n kiá»ƒm tra giáº£ Ä‘á»‹nh, thiáº¿t káº¿ nghiÃªn cá»©u vÃ  mÃ£ hoÃ¡ biáº¿n Ä‘á»ƒ diá»…n giáº£i Ä‘Ãºng."
)
